
@online{Medium,
    author = "Emily Smith",
    title = "Machine Learning for Malware Detection: Techniques, Models, and Industry Impact",
    url  = "https://medium.com/@smith.emily2584/machine-learning-for-malware-detection-techniques-models-and-industry-impact-80f17472844e",
    addendum = "(accessed: 12.11.2025)"
}
@online{DataScientest,
    author = "DataScientest",
    title = "Adversarial Attack: Definition and protection against this threat",
    url  = "https://datascientest.com/en/adversarial-attack-definition-and-protection-against-this-threat",
    addendum = "(accessed: 12.11.2025)"
}
@online{GeeksforGeeks,
    author = "GeeksforGeeks ",
    title = "Differences between Black Box Testing and White Box Testing",
    url  = "https://www.geeksforgeeks.org/software-testing/differences-between-black-box-testing-vs-white-box-testing/",
    addendum = "(accessed: 12.11.2025)"
}





@conference{kaushik_machine_2023,
    author ={Kaushik, Nupur and Bhardwaj, Vinay and Arri, Harwant Singh},
    booktitle = {2023 14th International Conference on Computing Communication and Networking Technologies ({ICCCNT})},
    title = {A Machine Learning-Based Survey Of Adversarial Attacks And Defenses In Malware Classification},
    year = {2023}
}

@article{costa_how_2024,
	title = {How Deep Learning Sees the World: A Survey on Adversarial Attacks \& Defenses},
	volume = {12},
	rights = {https://creativecommons.org/licenses/by-nc-nd/4.0/},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/10510296/},
	doi = {10.1109/ACCESS.2024.3395118},
	shorttitle = {How Deep Learning Sees the World},
	pages = {61113--61136},
	journaltitle = {{IEEE} Access},
	shortjournal = {{IEEE} Access},
	author = {Costa, Joana C. and Roxo, Tiago and Proença, Hugo and Inácio, Pedro Ricardo Morais},
	urldate = {2024-05-30},
	date = {2024},
	file = {Submitted Version:/Users/dannykim/Zotero/storage/UJX36RK3/Costa et al. - 2024 - How Deep Learning Sees the World A Survey on Adve.pdf:application/pdf},
}

@inproceedings{qi_adversarial_2022,
	location = {Changsha, China},
	title = {Adversarial Example Attacks Against Intelligent Malware Detection: A Survey},
	rights = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-66546-265-5},
	url = {https://ieeexplore.ieee.org/document/10056478/},
	doi = {10.1109/ICAML57167.2022.00068},
	shorttitle = {Adversarial Example Attacks Against Intelligent Malware Detection},
	eventtitle = {2022 4th International Conference on Applied Machine Learning ({ICAML})},
	pages = {1--7},
	booktitle = {2022 4th International Conference on Applied Machine Learning ({ICAML})},
	publisher = {{IEEE}},
	author = {Qi, Xuyan and Tang, Yonghe and Wang, Huanwei and Liu, Tieming and Jing, Jing},
	urldate = {2024-05-30},
	date = {2022-07},
}

@article{wang_threats_2023,
	title = {Threats to Training: A Survey of Poisoning Attacks and Defenses on Machine Learning Systems},
	volume = {55},
	issn = {0360-0300, 1557-7341},
	url = {https://dl.acm.org/doi/10.1145/3538707},
	doi = {10.1145/3538707},
	shorttitle = {Threats to Training},
	abstract = {Machine learning ({ML}) has been universally adopted for automated decisions in a variety of fields, including recognition and classification applications, recommendation systems, natural language processing, and so on. However, in light of high expenses on training data and computing resources, recent years have witnessed a rapid increase in outsourced {ML} training, either partially or completely, which provides vulnerabilities for adversaries to exploit. A prime threat in training phase is called poisoning attack, where adversaries strive to subvert the behavior of machine learning systems by poisoning training data or other means of interference. Although a growing number of relevant studies have been proposed, the research among poisoning attack is still overly scattered, with each paper focusing on a particular task in a specific domain. In this survey, we summarize and categorize existing attack methods and corresponding defenses, as well as demonstrate compelling application scenarios, thus providing a unified framework to analyze poisoning attacks. Besides, we also discuss the main limitations of current works, along with the corresponding future directions to facilitate further researches. Our ultimate motivation is to provide a comprehensive and self-contained survey of this growing field of research and lay the foundation for a more standardized approach to reproducible studies.},
	pages = {1--36},
	number = {7},
	journaltitle = {{ACM} Computing Surveys},
	shortjournal = {{ACM} Comput. Surv.},
	author = {Wang, Zhibo and Ma, Jingjing and Wang, Xue and Hu, Jiahui and Qin, Zhan and Ren, Kui},
	urldate = {2024-05-30},
	date = {2023-07-31},
	langid = {english},
}


@online{Nightfall,
    author = "Nightfall",
    title = "Evasion Attacks",
    url  = "https://www.nightfall.ai/ai-security-101/evasion-attacks",
    addendum = "(accessed: 12.11.2025)",
}


@article{lundberg2017,
    author = "Scott Lundberg and Su-In Lee",
    title ="A Unified Approach to Interpreting Model Predictions" ,
    journal = "Arxiv",
    year = "2017"
}


@online{0xrick,
    author = "0xRick ",
    title = "A dive into the PE file format - PE file structure - Part 1: overview",
    url  = "https://0xrick.github.io/win-internals/pe2/",
    addendum = "(accessed: 12.11.2025)",
}



@article{Anderson2018,
    author = "Hyrum S. Anderson and Anant Kharkar and Bobby Filar and David Evans and Phil Roth",
    title ="Learning to Evade Static PE Machine Learning Malware Models via Reinforcement Learning",
    journal = "Arxiv",
    year = "2018"
}


@online{geeksforgeeksLGBM,
    author = "GeeksforGeeks ",
    title = "LightGBM (Light Gradient Boosting Machine)",
    url  = "https://www.geeksforgeeks.org/machine-learning/lightgbm-light-gradient-boosting-machine/",
    addendum = "(accessed: 12.11.2025)"
}




@online{geeksforgeeksfeatureextraction,
    author = "GeeksforGeeks ",
    title = "What is Feature Extraction?",
    url  = "https://www.geeksforgeeks.org/machine-learning/what-is-feature-extraction/",
    addendum = "(accessed: 12.11.2025)"
}


@article{Aryal2024,
    author = "Kshitiz Aryal and Maanak Gupta and Mahmoud Abdelsalam and Moustafa Saleh",
    title ="Explainability Guided Adversarial Evasion Attacks on Malware Detectors" ,
    journal = "Arxiv",
    year = "2024"
}

@article{castro2019,
    author = "Raphael Labaca Castro and Corinna Schmitt and Gabi Dreo Rodosek",
    title ="ARMED: How Automatic Malware Modifications Can Evade Static Detection?" ,
    journal = "ResearchGate",
    year = "2019"
}

@ARTICLE{2018arXiv180404637A,
  author = {{Anderson}, H.~S. and {Roth}, P.},
  title = "{EMBER: An Open Dataset for Training Static PE Malware Machine Learning Models}",
  journal = {ArXiv e-prints},
  archivePrefix = "arXiv",
  eprint = {1804.04637},
  primaryClass = "cs.CR",
  keywords = {Computer Science - Cryptography and Security},
  year = 2018,
  month = apr,
  adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180404637A},
}

@article{Anderson2018,
    author = "Hyrum S. Anderson and Anant Kharkar and Bobby Filar and David Evans and Phil Roth",
    title ="Learning to Evade Static PE Machine Learning Malware Models
via Reinforcement Learning",
    journal = "Arxiv",
    year = "2018"
}


@ARTICLE{anderson2018learning,
  author={Anderson, Hyrum S and Kharkar, Anant and Filar, Bobby and Evans, David and Roth, Phil},
  title={Learning to Evade Static PE Machine Learning Malware Models via Reinforcement Learning},
  journal={arXiv preprint arXiv:1801.08917},
  archivePrefix = "arXiv",
  eprint = {1801.08917},
  primaryClass = "cs.CR",
  keywords = {Computer Science - Cryptography and Security},
  year = 2018,
  month = jan,
  adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180108917A},
}

@article{Aryal2023,
    author = "Kshitiz Aryal and Maanak Gupta and Mahmoud Abdelsalam",
    title ="Exploiting Windows PE Structure for
Adversarial Malware Evasion Attacks",
    journal = "ACM Digital Library",
    year = "2023"
}

@article{Kolosnjaji2018,
    author = "Bojan Kolosnjaji and Ambra Demontis and Battista Biggio and David maiorca and Giorgio Giacinto and Claudia Eckert and Fabio Roli",
    title ="Adversarial Malware Binaries: Evading Deep Learning for Malware Detection in Executables" ,
    journal = "arxiv",
    year = "2018"
}
@article{luca_demetrio_functionality-preserving_2021,
	title = {Functionality-preserving Black-box Optimization of Adversarial Windows Malware},
	volume = {16},
	issn = {1556-6013, 1556-6021},
	url = {http://arxiv.org/abs/2003.13526},
	doi = {10.1109/TIFS.2021.3082330},
	abstract = {Windows malware detectors based on machine learning are vulnerable to adversarial examples, even if the attacker is only given black-box query access to the model. The main drawback of these attacks is that: (i) they are query-inefficient, as they rely on iteratively applying random transformations to the input malware; and (ii) they may also require executing the adversarial malware in a sandbox at each iteration of the optimization process, to ensure that its intrusive functionality is preserved. In this paper, we overcome these issues by presenting a novel family of black-box attacks that are both query-efficient and functionality-preserving, as they rely on the injection of benign content - which will never be executed - either at the end of the malicious file, or within some newly-created sections. Our attacks are formalized as a constrained minimization problem which also enables optimizing the trade-off between the probability of evading detection and the size of the injected payload. We empirically investigate this trade-off on two popular static Windows malware detectors, and show that our black-box attacks can bypass them with only few queries and small payloads, even when they only return the predicted labels. We also evaluate whether our attacks transfer to other commercial antivirus solutions, and surprisingly find that they can evade, on average, more than 12 commercial antivirus engines. We conclude by discussing the limitations of our approach, and its possible future extensions to target malware classifiers based on dynamic analysis.},
	pages = {3469--3478},
	journaltitle = {{IEEE} Transactions on Information Forensics and Security},
	shortjournal = {{IEEE} Trans.Inform.Forensic Secur.},
	author = {{Luca Demetrio} and {Battista Biggio} and {Giovanni Lagorio} and {Fabio Roli} and {Alessandro Armando}},
	urldate = {2024-06-04},
	date = {2021},
	eprinttype = {arxiv},
	eprint = {2003.13526 [cs]},
	keywords = {Computer Science - Cryptography and Security, Windows {PE} malware, Preserved function, {MalConv}, {GAMMA}, {VirusTotal}, {GBD}, Padding attacks, Section Injection attack},
	file = {arXiv Fulltext PDF:/Users/dannykim/Zotero/storage/MP29FA9C/Demetrio et al. - 2021 - Functionality-preserving Black-box Optimization of.pdf:application/pdf;arXiv.org Snapshot:/Users/dannykim/Zotero/storage/LP96UVK2/2003.html:text/html},
}

@online{duelusetechnology,
    author = "Bita Afshar and Kash Khorasani ",
    title = "Briefing Notes",
    url  = "https://www.concordia.ca/content/dam/ginacody/research/spnet/Documents/BriefingNotes/EmergingTech-MilitaryApp/BN-19-Emerging-technology-and-military-application-Oct2020.pdf",
    addendum = "(accessed: 12.11.2025)"
}


@online{NIH,
    author = "National Institutes of Health",
    title = "Duel-Use Research",
    url  = "https://oir.nih.gov/sourcebook/ethical-conduct/special-research-considerations/dual-use-research",
    addendum = "(accessed: 12.11.2025)",
}